---
title: "DEM2DTM workflow"
author: "Andrii Zaiats"
date: "Last updated: 2022-08-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = TRUE, eval = FALSE)
```

#### DEM to DTM workflow for SfM dense clouds using R and CloudCompare.

The following code represents a workflow that may undergo changes/improvements. Cautions: (a) from one step in CloudCompare to the next, the number of tiles may be reduced due to the really tiny tiles representing the edges; similar reduction of tile number may be due to too aggressive filtering parameters; (b) the current implementation of CloudCompare's CSF function in CLI may result in the output tiles placed in a different location, - these may need to be located and transferred to the project folder manually; (c) if the following code chunk that calls CloudCompare is ran before the current process finishes in the background.

##### *Steps in the workflow:*

1.  Split dense point cloud into smaller tiles (as an option, we can choose to process the entire file - details below).

2.  Select a few tiles for test runs and parameter tuning (optional but recommended). It may also be fruitful to use CC's GUI to execute the filtering steps on a test file for faster parameter tuning.

3.  Use CloudCompare (CC) to clean DEM down to "*ground points*" only:

    -   Apply CSF algorithm to remove large vegetation chunks;\
    -   Apply SOR filter to remove outliers;\
    -   Calculate Normal Rate change (*e.i.*, curvature): large neighborhood;\
    -   Calculate Normal Rate change (*e.i.*, curvature): small neighborhood;\
    -   Calculate Normals;\
    -   Merge tiles into a single *.las* file;\
    -   Fit Poisson surface and filter points by distance to mesh;\
    -   Calculate Normals;\
    -   Generate final DTM surface.

4.  Generate DTM raster.

5.  Generate CHM.

##### *Required software*

-   R packages: *lidR, future, raster, sf, mapview, mapedit, terra*.

-   CloudCompare version 2.12+

A good practice would be to create a separate folder for each dense cloud to be processed. For example, the following structure would work:

> *Project folder*:\
> \|- *denseCloud.las*\
> \|- *dem.tif*\
> \|- *tiles* [empty folder]\
> \|- *test* [empty folder]

Setting up the environment: first we load the R packages, then provide system paths to the input file(s) and where the output should be stored. The three paths are: (1) path to the location of the dense cloud [*Project folder* above], (2) path to the *tiles* folder, and (3) path to the *test* folder. Lastly, we can specify site name, date of data collection, and CRS for consistent naming convention.

##### Set up project environment

```{r, results='hide', message=FALSE}
# load packages
pkgs <- c("lidR", "future", "sf", "mapview", "mapedit", "terra", "devtools")
sapply(pkgs, require, character.only = TRUE, quietly = TRUE)

devtools::source_url("https://github.com/andriizayac/uas_data_preprocess/blob/main/utils/merge_tiles_fn.R?raw=TRUE")

# specify paths
path_prj <- "path/to/projectfolder/with/denseCloud"
path_tiles <- paste0(path_prj, "tiles/")
path_test <- paste0(path_prj, "test/")

dir.create(path_tiles, showWarnings = FALSE)
dir.create(path_test, showWarnings = FALSE)

# data attributes
site <- "" # 
date <- "" # in YYYYMMDD format
prj_crs <- "" # e.g. wgs84utm11n
```

*A note on coordinate reference systems:* Nowhere in this code we apply CRS transformations. The output DTM raster will be in the same CRS as the input dense cloud. The dense cloud needs to be in a projected CRS (*e.g.*, wgs84utm11n) so that the *x, y, z* are all on the same scale.

#### 1. Split dense point cloud into smaller tiles.

*The workflow assumes we have a single .las file output from the SfM software. However,\
- if we have a collection of .laz/.las files that need to be merged first (default WebODM output inlcludes multiple small tiles with different extents and densities) before processing or tiling, we can run the merge function first (see the end of the script).\
- If we already have a collection of tiles in a .las format and want to process them separately, we need move them to the **tiles** folder and carry on to **Step 3***.

We can adjust the tile size of tiles using the `opt_chunk_size()` function as necessary. In case our input *.las* file is small enough to be processed as a single file, we can just specify the tile size really large (beyond the footprint of the dense cloud) and proceed with the workflow (in this case we can skip **Step 2**). On the other hand, if the file is large, this step may take a few minutes to tile up the main dense cloud, and the rest of the code will process each tile separately. For UAS datasets \~1 cm resolution, a tile size of 10 m is an adequate starting point.

```{r, message=FALSE}
file <- list.files(path_prj, pattern = "\\.las$", full.names = TRUE)
las = readLAScatalog(file) # here 'file' should be a single  DenseCloud, otherwise prompts an error

tile_size <- 10 # units [meters]

opt_chunk_buffer(las) <- 0 # units [meters]
opt_chunk_size(las) <- tile_size # units [meters]
opt_output_files(las) <- paste0(path_tiles, "tile_{ID}")

library(future)
plan(multisession) # parallelize

tic <- Sys.time()
tiles = catalog_retile(las) # creates another catalog object and saves tiles on disc as well
toc <- tic - Sys.time()

plan(sequential) # close parallel setup
```

#### 2. Select a few tiles for test runs and parameter tuning.

We can skip this step if we know the parameters for CloudCompare filtering. Otherwise, this step may be handy and save time if we have a large dataset at hand. The `tile_size` should be same as in **Step 1**. To run the workflow on a subset of tiles, in ***Step 3.1*** we need to specify `Test = 1` in the beginning of the chunk, which will contain a subset of tiles we select here. Otherwise, we set `Test = 0`, to run the steps on a full dataset.

```{r, message=FALSE}
tile_size <- 10

tiles <- readLAScatalog(path_tiles) 
ts <- catalog_select(tiles)
opt_output_files(ts) <- paste0(path_test, "tile_{ID}")
opt_chunk_buffer(ts) <- 0
opt_chunk_size(ts) <- tile_size # same as the tile size
newts <- catalog_retile(ts)
```

#### 3. Use CloudCompare to clean DEM down to "*ground points*".

*For most of Step 3 the script runs CloudCompare commands via Command Line Interface (CLI).* *Note: while the script in each chunk may indicate that the program has finished after we ran it, it may actually still run in the background for a while in CloudCompare. Running the next step before the previous has finished may cause the loss of tiles. One way to keep an eye on the actual progress can be to open the project folder and check the times each chunk was created or modified, which should be consistent for all tiles.*

##### 3.1: Create a path for *CloudCompare* command line interface.

```{r}
Test = 1

if(Test == 1) {
  path <- path_test
} else {
  path <- path_tiles
}

wdir <- gsub("/", "\\\\", path) # specify where the tiles are 
```

##### 3.2: Apply CSF for initial separation of Ground and Vegetation, *e.g.,* [-CSF -SCENES SLOPE -CLOTH_RESOLUTION 0.1 -MAX_ITERATION 500 -CLASS_THRESHOLD 0.2 -EXPORT_GROUND]

*note: the CloudCompare CSF function is new and may be unstable when ran from CLI - the output may be placed in unexpected directory on a hard drive. The following chunk includes a line to search for the output and place it back into the project working directory, but providing an approximate path where files are stored, `search_dir` will speed it up. It'd be best to provide a search directory, if the search runs on the entire hard drive it will take a long time and may accidentally grab unrelated files. Alternatively, feel free to out-comment the search-relocation code lines and transfer the files manually.*

This step runs a command to separate ground from vegetation objects. The objective of this step is to coarsely split vegetation from ground point, erring on the conservative side as the subsequent steps will chip away vegetation from ground more carefully. CSF (Cloth Simulation Filter) is set up to run with a "SLOPE" setting, which allows the algorithm to better separate vegetation from ground on complex topography, but should be changed to 'FLAT' on even terrain. Two other arguments that can be modified to adjust the output is "CLOTH RESOLUTION" and "CLASS THRESHOLD", best way would be to experiment with different values in CloudCompare based on a point cloud at hand.

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*.las) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -CSF -SCENES FLAT -CLOTH_RESOLUTION 0.1 -MAX_ITERATION 500 -CLASS_THRESHOLD 0.1 -EXPORT_GROUND -NO_TIMESTAMP -C_EXPORT_FMT BIN")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)

search_dir <- "D:/Andrii/"

lf <- list.files(search_dir, pattern = "tile.*_ground_points.bin", recursive=TRUE, full.names=TRUE)

# this loop ensures that we move ground points only when CSF command finshed running in the background
while(length(lf) < length(list.files(path, pattern = ".las$"))) {
  Sys.sleep(10)
  
  lf <- list.files(search_dir, pattern = "tile.*_ground_points.bin", recursive=TRUE, full.names=TRUE)
} 

lfnew <- paste0(path, sapply(strsplit(lf, "/"), function(x) { x[length(x)]})) 
file.rename(lf, lfnew)
```

##### 3.3: Denoise using SOR filter, *e.g.,* [-SOR 100 2].

This step runs a command to clean the point cloud from outliers. SOR (Statistical Outlier Filter) takes in two arguments: (1) number of neighbors (specifies a radius to search for outliers), and (2) the number of standard deviations as a measure distance from the mean to create a threshold for point removal. Larger values in (1) will take longer to run and will consider larger neighborhoods to determine points that represent noise, and smaller values in (2) will set a more aggressive threshold.

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*_ground_points.bin) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -SOR 100 2 -NO_TIMESTAMP -C_EXPORT_FMT BIN -SAVE_CLOUDS")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)
```

##### 3.4: Calculate Normal Change Rate (*i.e.*, curvature) at larger neighborhoods, *e.g.,* [-CURV NORMAL_CHANGE 0.25].

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*_ground_points_SOR.bin) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -CURV NORMAL_CHANGE 0.25 -NO_TIMESTAMP -C_EXPORT_FMT BIN -SAVE_CLOUDS")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)
```

##### 3.5: Filter DenseCloud based on calculated curvatures at some threshold, *e.g.,* [-FILTER_SF MIN 0.01].

This step filters out points that are above the specified curvature value. Higher values will result in retaining more points, while smaller values will remove points more aggressively. Setting the filter value too low may result in a dense cloud with large gaps, which will make surface reconstruction more difficult and likely less accurate when interpolating underneath the shrub canopies. Best to use CloudCompare GUI to fine-tune the threshold for a point cloud at hand.

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*_ground_points_SOR.bin) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -FILTER_SF MIN 0.01 -NO_TIMESTAMP -C_EXPORT_FMT BIN -SAVE_CLOUDS")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)
```

##### 3.6: Calculate Normal Change Rate at smaller neighborhoods, *e.g.,* [-CURV NORMAL_CHANGE 0.2].

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*_SOR_FILTERED_*.bin) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -CURV NORMAL_CHANGE 0.2 -NO_TIMESTAMP -C_EXPORT_FMT BIN -SAVE_CLOUDS")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)
```

##### 3.7: Filter the cloud based on the calculated curvatures at some threshold, *e.g.,* [-FILTER_SF MIN 0.01].

This step filters out points that are above the specified curvature value. The difference from **Step 3.5** is that the curvatures are calculated given smaller neighborhoods, filtering out smaller-scale features.

```{r}
# CloudCompare command
cmd <- paste0("FOR %f IN (", wdir, "*_SOR_FILTERED_*.bin) DO \"C:/Program Files/CloudCompare/cloudcompare.exe\" -SILENT -O -GLOBAL_SHIFT AUTO %f -AUTO_SAVE OFF -FILTER_SF MIN 0.01 -NO_TIMESTAMP -C_EXPORT_FMT LAS -SAVE_CLOUDS")

system("cmd.exe", input = cmd, show.output.on.console = FALSE)
```

##### 3.8: Merge the final ground tiles.

A reverse step that combines all tiles into a single *.las* file for final steps in CloudCompare GUI. The output is saved in the project path, named *merge_ground.las*.

```{r, results='hide'}
# first remove all intermediate products .bin
lf <- list.files(path, full.names = TRUE)
file.remove(lf[grepl(".*.bin$", lf)])

# temporary create a copy of the final .las files 
lf <- list.files(path, pattern = "\\].las$", full.names = TRUE)

# temp folder
dir.create(paste0(path, "tmp"), showWarnings = FALSE)
Sys.sleep(2)
file.copy(lf, gsub(path, paste0(path, "tmp/"), lf))
Sys.sleep(5)

# wait until the files transfer
if(length(lf) > length(list.files(paste0(path, "/tmp/"), pattern = "\\].las$"))) {
  Sys.sleep(10)
  
  lf <- list.files(path, pattern = "\\].las$", full.names = TRUE)
} 

# merge the final ground tiles
merge_tiles(paste0(path, "tmp"), paste0(path_prj, "merge_ground"))
# delete temporary directory
unlink(paste0(path, "tmp"), recursive = TRUE)
```

##### 

> If we're running the script on a subset of tiles, this may be a good stopping point. Explore how the output surface looks and whether the filtered point cloud does not have any excessively large gaps that may result in concave/convex interpolation (if Poisson interpolation is used). If the output looks fine, we can go back to **Step 3.1** and change the path to `path_tiles` that includes the full dataset.
>
> Update: triangulation is more robust to large gaps in the remaining ground point cloud than Poisson, it's less sensitive to curvatures along plant margins that failed to be filtered out, and the resultant triangulated surface can be smoothed out to remove sharp elevation changes in DTM.

#### 4. Generate DTM raster.

The final steps to produce a DTM and subsequently use it to calculate CHM will use the *lidR* package to interpolate the ground surface between the remaining points in the filtered point cloud (*i.e.* output from **Step 3.8**). The chunk of code below implements the following operations:

-   Assigns all remaining points as *ground points*.

-   Interpolates the gaps using Triangular Irregular Network (TIN) algorithm.

-   Discards ground points that are more than 5 cm below and 5 cm above the interpolated surface.

-   Re-create the surface using the same TIN algorithm.

-   Saves the output as: (1) csv file with X, Y, Z, DTM points, and (2) a raster DTM file.

```{r}
f <- list.files(path_prj, pattern = "merge_ground.las", full.names = TRUE)
las <- readLAS(f)
las$Classification <- 2L

dtm0 <- rasterize_terrain(las, res = 0.5, algorithm = tin())

las <- add_attribute(las, normalize_height(las, dtm0)$Z, "dist")

las1 <- filter_poi(las, dist > -0.05 & dist < 0.05)

dtm1 <- rasterize_terrain(las1, res = 0.3, algorithm = tin()) |>
  disagg(fact = 10, method = "bilinear") |>
  focal(w = 15, fun = "mean")

pts <- as.data.frame(dtm1, xy = TRUE)
write.csv(pts, file = paste0(path_prj, "ground_pts.csv"), row.names = FALSE)

writeRaster(dtm1, file = paste0(path_prj, site, date, "_dtm_", prj_crs, ".tif"), overwrite = TRUE)
```

#### 5. Generate CHM (optionally).

```{r, results='hide', warning=FALSE, warning=TRUE}
f <- list.files(path_prj, pattern = "^dsm.tif$", recursive = TRUE, full.names = TRUE)[1]
dem <- rast(f)

f <- list.files(path_prj, pattern = "_dtm_.*.tif$", full.names = TRUE)
dtm <- rast(f)

dem <- crop(dem, ext(dtm))
dtm <- resample(dtm, dem)

chm <- dem - dtm

writeRaster(chm, file = paste0(path_prj, site, "_", date, "_chm_", prj_crs, ".tif"), overwrite = TRUE)
```

------------------------------------------------------------------------

#### Merge WebODM output.

This code chunk may be useful if we have hundreds of .laz/.las files that need to be merged before processing. For example, the default WebODM output includes a stack of .laz files that vary in their extent size and point density. These chunks need to be merged first to obtain the dense cloud, and then tiled if necessary.

```{r}
path_odm_chunks <- "path/to/cloud/chunks"

file_name <- paste(site, date, "_denseCloud_", prj_crs, sep = "_")

merge_tiles(path_odm_chunks, paste0(path_prj, file_name))
```
